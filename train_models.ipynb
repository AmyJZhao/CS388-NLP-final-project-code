{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP83mIw44DTRW6+XzrN1stN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmyJZhao/CS388-NLP-final-project-code/blob/main/train_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ELECTRA model training\n",
        "Datasets:\n",
        "* SQUAD: https://huggingface.co/datasets/squad\n",
        "* SQUAD 2.0: https://huggingface.co/datasets/squad_v2\n",
        "* SQUAD Adversarial (AddSent and AddOneSent): https://huggingface.co/datasets/squad_adversarial\n",
        "\n"
      ],
      "metadata": {
        "id": "RqA5FcK7uoxW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ev9tMVD5U24",
        "outputId": "216ed3bf-deca-474d-d325-8c9c3d754186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/fp-model-train\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.23.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (4.23.0)\n",
            "Requirement already satisfied: datasets==2.6.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.12.1+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.0->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.0->-r requirements.txt (line 1)) (0.11.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.0->-r requirements.txt (line 1)) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.0->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.0->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.0->-r requirements.txt (line 1)) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.0->-r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.0->-r requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets==2.6.1->-r requirements.txt (line 2)) (1.3.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets==2.6.1->-r requirements.txt (line 2)) (0.70.13)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.6.1->-r requirements.txt (line 2)) (2022.11.0)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.8/dist-packages (from datasets==2.6.1->-r requirements.txt (line 2)) (0.3.5.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.6.1->-r requirements.txt (line 2)) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets==2.6.1->-r requirements.txt (line 2)) (3.8.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets==2.6.1->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets==2.6.1->-r requirements.txt (line 2)) (0.18.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.6.1->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.6.1->-r requirements.txt (line 2)) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.6.1->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.6.1->-r requirements.txt (line 2)) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.6.1->-r requirements.txt (line 2)) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.6.1->-r requirements.txt (line 2)) (1.8.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.6.1->-r requirements.txt (line 2)) (4.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.23.0->-r requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.23.0->-r requirements.txt (line 1)) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.23.0->-r requirements.txt (line 1)) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.23.0->-r requirements.txt (line 1)) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.23.0->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.23.0->-r requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==2.6.1->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==2.6.1->-r requirements.txt (line 2)) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==2.6.1->-r requirements.txt (line 2)) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/fp-model-train\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SQUAD"
      ],
      "metadata": {
        "id": "GnasvcAUvRqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run.py --do_train --task qa --dataset squad --output_dir ./models/squad/ "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHyv5dSytHOD",
        "outputId": "aef890d9-cb28-4c4d-8f79-d7c53b2d69fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-05 00:46:21.202252: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "Downloading builder script: 100% 5.27k/5.27k [00:00<00:00, 5.84MB/s]\n",
            "Downloading metadata: 100% 2.36k/2.36k [00:00<00:00, 2.65MB/s]\n",
            "Downloading readme: 100% 7.67k/7.67k [00:00<00:00, 8.32MB/s]\n",
            "Downloading and preparing dataset squad/plain_text to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n",
            "Downloading data files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/8.12M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:  93% 7.53M/8.12M [00:00<00:00, 75.2MB/s]\u001b[A\n",
            "Downloading data: 17.5MB [00:00, 89.4MB/s]                \u001b[A\n",
            "Downloading data: 30.3MB [00:00, 91.8MB/s]\n",
            "Downloading data files:  50% 1/2 [00:00<00:00,  2.13it/s]\n",
            "Downloading data: 4.85MB [00:00, 73.6MB/s]       \n",
            "Downloading data files: 100% 2/2 [00:00<00:00,  2.86it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1539.76it/s]\n",
            "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 313.18it/s]\n",
            "Downloading: 100% 665/665 [00:00<00:00, 642kB/s]\n",
            "Downloading: 100% 54.2M/54.2M [00:01<00:00, 28.5MB/s]\n",
            "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
            "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 35.2kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 1.96MB/s]\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 3.43MB/s]\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "#0:   0% 0/44 [00:00<?, ?ba/s]\n",
            "#1:   0% 0/44 [00:00<?, ?ba/s]\u001b[A\n",
            "#0:   2% 1/44 [00:00<00:36,  1.17ba/s]\n",
            "#0:   7% 3/44 [00:02<00:31,  1.31ba/s]\n",
            "#0:   9% 4/44 [00:03<00:29,  1.37ba/s]\n",
            "#0:  11% 5/44 [00:03<00:28,  1.38ba/s]\n",
            "#0:  14% 6/44 [00:04<00:29,  1.28ba/s]\n",
            "#0:  16% 7/44 [00:05<00:28,  1.32ba/s]\n",
            "#0:  18% 8/44 [00:05<00:26,  1.37ba/s]\n",
            "#0:  20% 9/44 [00:06<00:25,  1.36ba/s]\n",
            "#0:  23% 10/44 [00:07<00:24,  1.40ba/s]\n",
            "#0:  25% 11/44 [00:08<00:22,  1.45ba/s]\n",
            "#0:  27% 12/44 [00:08<00:21,  1.47ba/s]\n",
            "#0:  30% 13/44 [00:09<00:21,  1.41ba/s]\n",
            "#0:  34% 15/44 [00:10<00:19,  1.49ba/s]\n",
            "#0:  36% 16/44 [00:11<00:18,  1.53ba/s]\n",
            "#0:  39% 17/44 [00:12<00:17,  1.54ba/s]\n",
            "#0:  41% 18/44 [00:12<00:16,  1.57ba/s]\n",
            "#0:  43% 19/44 [00:13<00:16,  1.55ba/s]\n",
            "#0:  45% 20/44 [00:14<00:16,  1.47ba/s]\n",
            "#0:  50% 22/44 [00:15<00:14,  1.51ba/s]\n",
            "#0:  52% 23/44 [00:15<00:13,  1.53ba/s]\n",
            "#0:  55% 24/44 [00:16<00:13,  1.48ba/s]\n",
            "#0:  57% 25/44 [00:17<00:12,  1.51ba/s]\n",
            "#0:  59% 26/44 [00:18<00:11,  1.50ba/s]\n",
            "#0:  61% 27/44 [00:18<00:12,  1.35ba/s]\n",
            "#0:  64% 28/44 [00:19<00:11,  1.35ba/s]\n",
            "#0:  66% 29/44 [00:20<00:11,  1.34ba/s]\n",
            "#0:  68% 30/44 [00:21<00:10,  1.33ba/s]\n",
            "#0:  70% 31/44 [00:21<00:09,  1.31ba/s]\n",
            "#0:  73% 32/44 [00:22<00:09,  1.30ba/s]\n",
            "#0:  75% 33/44 [00:23<00:08,  1.24ba/s]\n",
            "#0:  77% 34/44 [00:24<00:07,  1.26ba/s]\n",
            "#0:  80% 35/44 [00:25<00:07,  1.27ba/s]\n",
            "#0:  82% 36/44 [00:25<00:06,  1.28ba/s]\n",
            "#0:  84% 37/44 [00:26<00:05,  1.28ba/s]\n",
            "#0:  86% 38/44 [00:27<00:04,  1.29ba/s]\n",
            "#1:  82% 36/44 [00:27<00:06,  1.32ba/s]\u001b[A\n",
            "#0:  89% 39/44 [00:28<00:03,  1.28ba/s]\n",
            "#0:  91% 40/44 [00:29<00:03,  1.22ba/s]\n",
            "#0:  93% 41/44 [00:29<00:02,  1.25ba/s]\n",
            "#0:  95% 42/44 [00:30<00:01,  1.28ba/s]\n",
            "#0:  98% 43/44 [00:32<00:00,  1.34ba/s]\n",
            "\n",
            "#1:  95% 42/44 [00:32<00:01,  1.30ba/s]\u001b[A\n",
            "#1:  98% 43/44 [00:33<00:00,  1.29ba/s]\n",
            "run.py:133: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = datasets.load_metric('squad')\n",
            "Downloading builder script: 4.50kB [00:00, 4.79MB/s]       \n",
            "Downloading extra modules: 3.31kB [00:00, 3.59MB/s]       \n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 87714\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 32895\n",
            "  0% 0/32895 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 3.1941, 'learning_rate': 4.9240006079951365e-05, 'epoch': 0.05}\n",
            "  2% 500/32895 [00:40<41:06, 13.14it/s]Saving model checkpoint to ./models/squad/checkpoint-500\n",
            "Configuration saved in ./models/squad/checkpoint-500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 2.0307, 'learning_rate': 4.848001215990272e-05, 'epoch': 0.09}\n",
            "  3% 1000/32895 [01:19<40:30, 13.13it/s]Saving model checkpoint to ./models/squad/checkpoint-1000\n",
            "Configuration saved in ./models/squad/checkpoint-1000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 1.7435, 'learning_rate': 4.772001823985408e-05, 'epoch': 0.14}\n",
            "  5% 1500/32895 [01:58<40:21, 12.96it/s]Saving model checkpoint to ./models/squad/checkpoint-1500\n",
            "Configuration saved in ./models/squad/checkpoint-1500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 1.5834, 'learning_rate': 4.6960024319805444e-05, 'epoch': 0.18}\n",
            "  6% 2000/32895 [02:36<39:24, 13.07it/s]Saving model checkpoint to ./models/squad/checkpoint-2000\n",
            "Configuration saved in ./models/squad/checkpoint-2000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 1.5518, 'learning_rate': 4.620003039975681e-05, 'epoch': 0.23}\n",
            "  8% 2500/32895 [03:15<39:30, 12.82it/s]Saving model checkpoint to ./models/squad/checkpoint-2500\n",
            "Configuration saved in ./models/squad/checkpoint-2500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 1.4656, 'learning_rate': 4.544003647970816e-05, 'epoch': 0.27}\n",
            "  9% 3000/32895 [03:54<38:36, 12.90it/s]Saving model checkpoint to ./models/squad/checkpoint-3000\n",
            "Configuration saved in ./models/squad/checkpoint-3000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 1.3954, 'learning_rate': 4.4680042559659524e-05, 'epoch': 0.32}\n",
            " 11% 3500/32895 [04:33<37:27, 13.08it/s]Saving model checkpoint to ./models/squad/checkpoint-3500\n",
            "Configuration saved in ./models/squad/checkpoint-3500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 1.3928, 'learning_rate': 4.3920048639610886e-05, 'epoch': 0.36}\n",
            " 12% 4000/32895 [05:12<36:51, 13.07it/s]Saving model checkpoint to ./models/squad/checkpoint-4000\n",
            "Configuration saved in ./models/squad/checkpoint-4000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 1.3942, 'learning_rate': 4.316005471956224e-05, 'epoch': 0.41}\n",
            " 14% 4500/32895 [05:51<36:02, 13.13it/s]Saving model checkpoint to ./models/squad/checkpoint-4500\n",
            "Configuration saved in ./models/squad/checkpoint-4500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 1.3575, 'learning_rate': 4.240006079951361e-05, 'epoch': 0.46}\n",
            " 15% 5000/32895 [06:30<35:26, 13.12it/s]Saving model checkpoint to ./models/squad/checkpoint-5000\n",
            "Configuration saved in ./models/squad/checkpoint-5000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-5000/special_tokens_map.json\n",
            "{'loss': 1.303, 'learning_rate': 4.1640066879464966e-05, 'epoch': 0.5}\n",
            " 17% 5500/32895 [07:09<34:47, 13.12it/s]Saving model checkpoint to ./models/squad/checkpoint-5500\n",
            "Configuration saved in ./models/squad/checkpoint-5500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-5500/special_tokens_map.json\n",
            "{'loss': 1.3083, 'learning_rate': 4.088007295941633e-05, 'epoch': 0.55}\n",
            " 18% 6000/32895 [07:48<34:07, 13.14it/s]Saving model checkpoint to ./models/squad/checkpoint-6000\n",
            "Configuration saved in ./models/squad/checkpoint-6000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-6000/special_tokens_map.json\n",
            "{'loss': 1.3108, 'learning_rate': 4.0120079039367684e-05, 'epoch': 0.59}\n",
            " 20% 6500/32895 [08:26<33:49, 13.01it/s]Saving model checkpoint to ./models/squad/checkpoint-6500\n",
            "Configuration saved in ./models/squad/checkpoint-6500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-6500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-6500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-6500/special_tokens_map.json\n",
            "{'loss': 1.2974, 'learning_rate': 3.9360085119319046e-05, 'epoch': 0.64}\n",
            " 21% 7000/32895 [09:05<32:55, 13.11it/s]Saving model checkpoint to ./models/squad/checkpoint-7000\n",
            "Configuration saved in ./models/squad/checkpoint-7000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-7000/special_tokens_map.json\n",
            "{'loss': 1.2343, 'learning_rate': 3.860009119927041e-05, 'epoch': 0.68}\n",
            " 23% 7500/32895 [09:44<32:52, 12.87it/s]Saving model checkpoint to ./models/squad/checkpoint-7500\n",
            "Configuration saved in ./models/squad/checkpoint-7500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-7500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-7500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-7500/special_tokens_map.json\n",
            "{'loss': 1.2595, 'learning_rate': 3.784009727922177e-05, 'epoch': 0.73}\n",
            " 24% 8000/32895 [10:24<31:58, 12.98it/s]Saving model checkpoint to ./models/squad/checkpoint-8000\n",
            "Configuration saved in ./models/squad/checkpoint-8000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-8000/special_tokens_map.json\n",
            "{'loss': 1.2172, 'learning_rate': 3.708010335917313e-05, 'epoch': 0.78}\n",
            " 26% 8500/32895 [11:03<31:26, 12.93it/s]Saving model checkpoint to ./models/squad/checkpoint-8500\n",
            "Configuration saved in ./models/squad/checkpoint-8500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-8500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-8500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-8500/special_tokens_map.json\n",
            "{'loss': 1.214, 'learning_rate': 3.632010943912449e-05, 'epoch': 0.82}\n",
            " 27% 9000/32895 [11:42<30:55, 12.88it/s]Saving model checkpoint to ./models/squad/checkpoint-9000\n",
            "Configuration saved in ./models/squad/checkpoint-9000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-9000/special_tokens_map.json\n",
            "{'loss': 1.1642, 'learning_rate': 3.556011551907585e-05, 'epoch': 0.87}\n",
            " 29% 9500/32895 [12:21<29:53, 13.04it/s]Saving model checkpoint to ./models/squad/checkpoint-9500\n",
            "Configuration saved in ./models/squad/checkpoint-9500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-9500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-9500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-9500/special_tokens_map.json\n",
            "{'loss': 1.2175, 'learning_rate': 3.4800121599027206e-05, 'epoch': 0.91}\n",
            " 30% 10000/32895 [13:00<29:16, 13.03it/s]Saving model checkpoint to ./models/squad/checkpoint-10000\n",
            "Configuration saved in ./models/squad/checkpoint-10000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-10000/special_tokens_map.json\n",
            "{'loss': 1.1933, 'learning_rate': 3.404012767897857e-05, 'epoch': 0.96}\n",
            " 32% 10500/32895 [13:39<29:00, 12.87it/s]Saving model checkpoint to ./models/squad/checkpoint-10500\n",
            "Configuration saved in ./models/squad/checkpoint-10500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-10500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-10500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-10500/special_tokens_map.json\n",
            "{'loss': 1.1587, 'learning_rate': 3.328013375892993e-05, 'epoch': 1.0}\n",
            " 33% 11000/32895 [14:18<27:53, 13.09it/s]Saving model checkpoint to ./models/squad/checkpoint-11000\n",
            "Configuration saved in ./models/squad/checkpoint-11000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-11000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-11000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-11000/special_tokens_map.json\n",
            "{'loss': 0.991, 'learning_rate': 3.252013983888129e-05, 'epoch': 1.05}\n",
            " 35% 11500/32895 [14:57<27:22, 13.02it/s]Saving model checkpoint to ./models/squad/checkpoint-11500\n",
            "Configuration saved in ./models/squad/checkpoint-11500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-11500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-11500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-11500/special_tokens_map.json\n",
            "{'loss': 0.9933, 'learning_rate': 3.176014591883265e-05, 'epoch': 1.09}\n",
            " 36% 12000/32895 [15:36<26:31, 13.13it/s]Saving model checkpoint to ./models/squad/checkpoint-12000\n",
            "Configuration saved in ./models/squad/checkpoint-12000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-12000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-12000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-12000/special_tokens_map.json\n",
            "{'loss': 1.0109, 'learning_rate': 3.100015199878401e-05, 'epoch': 1.14}\n",
            " 38% 12500/32895 [16:15<26:02, 13.05it/s]Saving model checkpoint to ./models/squad/checkpoint-12500\n",
            "Configuration saved in ./models/squad/checkpoint-12500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-12500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-12500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-12500/special_tokens_map.json\n",
            "{'loss': 0.9741, 'learning_rate': 3.024015807873537e-05, 'epoch': 1.19}\n",
            " 40% 13000/32895 [16:54<25:11, 13.16it/s]Saving model checkpoint to ./models/squad/checkpoint-13000\n",
            "Configuration saved in ./models/squad/checkpoint-13000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-13000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-13000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-13000/special_tokens_map.json\n",
            "{'loss': 1.0088, 'learning_rate': 2.9480164158686728e-05, 'epoch': 1.23}\n",
            " 41% 13500/32895 [17:33<24:53, 12.99it/s]Saving model checkpoint to ./models/squad/checkpoint-13500\n",
            "Configuration saved in ./models/squad/checkpoint-13500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-13500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-13500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-13500/special_tokens_map.json\n",
            "{'loss': 0.9987, 'learning_rate': 2.8720170238638093e-05, 'epoch': 1.28}\n",
            " 43% 14000/32895 [18:11<24:00, 13.11it/s]Saving model checkpoint to ./models/squad/checkpoint-14000\n",
            "Configuration saved in ./models/squad/checkpoint-14000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-14000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-14000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-14000/special_tokens_map.json\n",
            "{'loss': 0.9795, 'learning_rate': 2.7960176318589452e-05, 'epoch': 1.32}\n",
            " 44% 14500/32895 [18:50<23:23, 13.11it/s]Saving model checkpoint to ./models/squad/checkpoint-14500\n",
            "Configuration saved in ./models/squad/checkpoint-14500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-14500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-14500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-14500/special_tokens_map.json\n",
            "{'loss': 1.0249, 'learning_rate': 2.7200182398540814e-05, 'epoch': 1.37}\n",
            " 46% 15000/32895 [19:29<22:35, 13.20it/s]Saving model checkpoint to ./models/squad/checkpoint-15000\n",
            "Configuration saved in ./models/squad/checkpoint-15000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-15000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-15000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-15000/special_tokens_map.json\n",
            "{'loss': 0.954, 'learning_rate': 2.6440188478492173e-05, 'epoch': 1.41}\n",
            " 47% 15500/32895 [20:07<21:56, 13.21it/s]Saving model checkpoint to ./models/squad/checkpoint-15500\n",
            "Configuration saved in ./models/squad/checkpoint-15500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-15500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-15500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-15500/special_tokens_map.json\n",
            "{'loss': 0.9473, 'learning_rate': 2.5680194558443532e-05, 'epoch': 1.46}\n",
            " 49% 16000/32895 [20:46<21:18, 13.22it/s]Saving model checkpoint to ./models/squad/checkpoint-16000\n",
            "Configuration saved in ./models/squad/checkpoint-16000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-16000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-16000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-16000/special_tokens_map.json\n",
            "{'loss': 1.0396, 'learning_rate': 2.4920200638394894e-05, 'epoch': 1.5}\n",
            " 50% 16500/32895 [21:24<20:37, 13.25it/s]Saving model checkpoint to ./models/squad/checkpoint-16500\n",
            "Configuration saved in ./models/squad/checkpoint-16500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-16500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-16500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-16500/special_tokens_map.json\n",
            "{'loss': 0.9895, 'learning_rate': 2.4160206718346253e-05, 'epoch': 1.55}\n",
            " 52% 17000/32895 [22:03<20:01, 13.23it/s]Saving model checkpoint to ./models/squad/checkpoint-17000\n",
            "Configuration saved in ./models/squad/checkpoint-17000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-17000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-17000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-17000/special_tokens_map.json\n",
            "{'loss': 0.9784, 'learning_rate': 2.3400212798297615e-05, 'epoch': 1.6}\n",
            " 53% 17500/32895 [22:41<19:23, 13.23it/s]Saving model checkpoint to ./models/squad/checkpoint-17500\n",
            "Configuration saved in ./models/squad/checkpoint-17500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-17500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-17500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-17500/special_tokens_map.json\n",
            "{'loss': 1.0324, 'learning_rate': 2.2640218878248974e-05, 'epoch': 1.64}\n",
            " 55% 18000/32895 [23:20<18:53, 13.14it/s]Saving model checkpoint to ./models/squad/checkpoint-18000\n",
            "Configuration saved in ./models/squad/checkpoint-18000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-18000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-18000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-18000/special_tokens_map.json\n",
            "{'loss': 0.931, 'learning_rate': 2.1880224958200336e-05, 'epoch': 1.69}\n",
            " 56% 18500/32895 [23:58<18:07, 13.24it/s]Saving model checkpoint to ./models/squad/checkpoint-18500\n",
            "Configuration saved in ./models/squad/checkpoint-18500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-18500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-18500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-18500/special_tokens_map.json\n",
            "{'loss': 0.9946, 'learning_rate': 2.11202310381517e-05, 'epoch': 1.73}\n",
            " 58% 19000/32895 [24:37<17:31, 13.21it/s]Saving model checkpoint to ./models/squad/checkpoint-19000\n",
            "Configuration saved in ./models/squad/checkpoint-19000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-19000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-19000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-19000/special_tokens_map.json\n",
            "{'loss': 0.9538, 'learning_rate': 2.0360237118103057e-05, 'epoch': 1.78}\n",
            " 59% 19500/32895 [25:15<16:56, 13.18it/s]Saving model checkpoint to ./models/squad/checkpoint-19500\n",
            "Configuration saved in ./models/squad/checkpoint-19500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-19500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-19500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-19500/special_tokens_map.json\n",
            "{'loss': 0.9915, 'learning_rate': 1.9600243198054416e-05, 'epoch': 1.82}\n",
            " 61% 20000/32895 [25:54<16:16, 13.20it/s]Saving model checkpoint to ./models/squad/checkpoint-20000\n",
            "Configuration saved in ./models/squad/checkpoint-20000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-20000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-20000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-20000/special_tokens_map.json\n",
            "{'loss': 0.9274, 'learning_rate': 1.884024927800578e-05, 'epoch': 1.87}\n",
            " 62% 20500/32895 [26:33<15:37, 13.22it/s]Saving model checkpoint to ./models/squad/checkpoint-20500\n",
            "Configuration saved in ./models/squad/checkpoint-20500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-20500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-20500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-20500/special_tokens_map.json\n",
            "{'loss': 0.9758, 'learning_rate': 1.8080255357957137e-05, 'epoch': 1.92}\n",
            " 64% 21000/32895 [27:11<15:03, 13.16it/s]Saving model checkpoint to ./models/squad/checkpoint-21000\n",
            "Configuration saved in ./models/squad/checkpoint-21000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-21000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-21000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-21000/special_tokens_map.json\n",
            "{'loss': 0.986, 'learning_rate': 1.7320261437908496e-05, 'epoch': 1.96}\n",
            " 65% 21500/32895 [27:50<14:28, 13.12it/s]Saving model checkpoint to ./models/squad/checkpoint-21500\n",
            "Configuration saved in ./models/squad/checkpoint-21500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-21500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-21500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-21500/special_tokens_map.json\n",
            "{'loss': 0.875, 'learning_rate': 1.6560267517859858e-05, 'epoch': 2.01}\n",
            " 67% 22000/32895 [28:28<13:43, 13.22it/s]Saving model checkpoint to ./models/squad/checkpoint-22000\n",
            "Configuration saved in ./models/squad/checkpoint-22000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-22000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-22000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-22000/special_tokens_map.json\n",
            "{'loss': 0.8129, 'learning_rate': 1.580027359781122e-05, 'epoch': 2.05}\n",
            " 68% 22500/32895 [29:07<13:26, 12.90it/s]Saving model checkpoint to ./models/squad/checkpoint-22500\n",
            "Configuration saved in ./models/squad/checkpoint-22500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-22500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-22500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-22500/special_tokens_map.json\n",
            "{'loss': 0.7614, 'learning_rate': 1.5040279677762578e-05, 'epoch': 2.1}\n",
            " 70% 23000/32895 [29:46<12:33, 13.13it/s]Saving model checkpoint to ./models/squad/checkpoint-23000\n",
            "Configuration saved in ./models/squad/checkpoint-23000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-23000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-23000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-23000/special_tokens_map.json\n",
            "{'loss': 0.7935, 'learning_rate': 1.4280285757713938e-05, 'epoch': 2.14}\n",
            " 71% 23500/32895 [30:25<11:50, 13.23it/s]Saving model checkpoint to ./models/squad/checkpoint-23500\n",
            "Configuration saved in ./models/squad/checkpoint-23500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-23500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-23500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-23500/special_tokens_map.json\n",
            "{'loss': 0.7857, 'learning_rate': 1.35202918376653e-05, 'epoch': 2.19}\n",
            " 73% 24000/32895 [31:03<11:21, 13.06it/s]Saving model checkpoint to ./models/squad/checkpoint-24000\n",
            "Configuration saved in ./models/squad/checkpoint-24000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-24000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-24000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-24000/special_tokens_map.json\n",
            "{'loss': 0.7599, 'learning_rate': 1.2760297917616659e-05, 'epoch': 2.23}\n",
            " 74% 24500/32895 [31:42<10:41, 13.10it/s]Saving model checkpoint to ./models/squad/checkpoint-24500\n",
            "Configuration saved in ./models/squad/checkpoint-24500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-24500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-24500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-24500/special_tokens_map.json\n",
            "{'loss': 0.7736, 'learning_rate': 1.200030399756802e-05, 'epoch': 2.28}\n",
            " 76% 25000/32895 [32:21<10:01, 13.13it/s]Saving model checkpoint to ./models/squad/checkpoint-25000\n",
            "Configuration saved in ./models/squad/checkpoint-25000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-25000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-25000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-25000/special_tokens_map.json\n",
            "{'loss': 0.7871, 'learning_rate': 1.1240310077519382e-05, 'epoch': 2.33}\n",
            " 78% 25500/32895 [32:59<09:25, 13.09it/s]Saving model checkpoint to ./models/squad/checkpoint-25500\n",
            "Configuration saved in ./models/squad/checkpoint-25500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-25500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-25500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-25500/special_tokens_map.json\n",
            "{'loss': 0.7697, 'learning_rate': 1.048031615747074e-05, 'epoch': 2.37}\n",
            " 79% 26000/32895 [33:38<08:45, 13.12it/s]Saving model checkpoint to ./models/squad/checkpoint-26000\n",
            "Configuration saved in ./models/squad/checkpoint-26000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-26000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-26000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-26000/special_tokens_map.json\n",
            "{'loss': 0.7858, 'learning_rate': 9.720322237422101e-06, 'epoch': 2.42}\n",
            " 81% 26500/32895 [34:16<08:02, 13.26it/s]Saving model checkpoint to ./models/squad/checkpoint-26500\n",
            "Configuration saved in ./models/squad/checkpoint-26500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-26500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-26500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-26500/special_tokens_map.json\n",
            "{'loss': 0.7906, 'learning_rate': 8.960328317373462e-06, 'epoch': 2.46}\n",
            " 82% 27000/32895 [34:55<07:25, 13.23it/s]Saving model checkpoint to ./models/squad/checkpoint-27000\n",
            "Configuration saved in ./models/squad/checkpoint-27000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-27000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-27000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-27000/special_tokens_map.json\n",
            "{'loss': 0.7733, 'learning_rate': 8.200334397324822e-06, 'epoch': 2.51}\n",
            " 84% 27500/32895 [35:34<06:52, 13.09it/s]Saving model checkpoint to ./models/squad/checkpoint-27500\n",
            "Configuration saved in ./models/squad/checkpoint-27500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-27500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-27500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-27500/special_tokens_map.json\n",
            "{'loss': 0.781, 'learning_rate': 7.440340477276183e-06, 'epoch': 2.55}\n",
            " 85% 28000/32895 [36:12<06:10, 13.20it/s]Saving model checkpoint to ./models/squad/checkpoint-28000\n",
            "Configuration saved in ./models/squad/checkpoint-28000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-28000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-28000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-28000/special_tokens_map.json\n",
            "{'loss': 0.7768, 'learning_rate': 6.680346557227543e-06, 'epoch': 2.6}\n",
            " 87% 28500/32895 [36:51<05:32, 13.21it/s]Saving model checkpoint to ./models/squad/checkpoint-28500\n",
            "Configuration saved in ./models/squad/checkpoint-28500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-28500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-28500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-28500/special_tokens_map.json\n",
            "{'loss': 0.7734, 'learning_rate': 5.920352637178903e-06, 'epoch': 2.64}\n",
            " 88% 29000/32895 [37:29<04:54, 13.24it/s]Saving model checkpoint to ./models/squad/checkpoint-29000\n",
            "Configuration saved in ./models/squad/checkpoint-29000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-29000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-29000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-29000/special_tokens_map.json\n",
            "{'loss': 0.7664, 'learning_rate': 5.160358717130263e-06, 'epoch': 2.69}\n",
            " 90% 29500/32895 [38:08<04:17, 13.19it/s]Saving model checkpoint to ./models/squad/checkpoint-29500\n",
            "Configuration saved in ./models/squad/checkpoint-29500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-29500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-29500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-29500/special_tokens_map.json\n",
            "{'loss': 0.7859, 'learning_rate': 4.400364797081624e-06, 'epoch': 2.74}\n",
            " 91% 30000/32895 [38:46<03:39, 13.16it/s]Saving model checkpoint to ./models/squad/checkpoint-30000\n",
            "Configuration saved in ./models/squad/checkpoint-30000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-30000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-30000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-30000/special_tokens_map.json\n",
            "{'loss': 0.7775, 'learning_rate': 3.6403708770329835e-06, 'epoch': 2.78}\n",
            " 93% 30500/32895 [39:25<03:01, 13.22it/s]Saving model checkpoint to ./models/squad/checkpoint-30500\n",
            "Configuration saved in ./models/squad/checkpoint-30500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-30500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-30500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-30500/special_tokens_map.json\n",
            "{'loss': 0.7803, 'learning_rate': 2.880376956984344e-06, 'epoch': 2.83}\n",
            " 94% 31000/32895 [40:04<02:24, 13.16it/s]Saving model checkpoint to ./models/squad/checkpoint-31000\n",
            "Configuration saved in ./models/squad/checkpoint-31000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-31000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-31000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-31000/special_tokens_map.json\n",
            "{'loss': 0.7785, 'learning_rate': 2.1203830369357045e-06, 'epoch': 2.87}\n",
            " 96% 31500/32895 [40:42<01:45, 13.21it/s]Saving model checkpoint to ./models/squad/checkpoint-31500\n",
            "Configuration saved in ./models/squad/checkpoint-31500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-31500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-31500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-31500/special_tokens_map.json\n",
            "{'loss': 0.7712, 'learning_rate': 1.3603891168870648e-06, 'epoch': 2.92}\n",
            " 97% 32000/32895 [41:21<01:07, 13.18it/s]Saving model checkpoint to ./models/squad/checkpoint-32000\n",
            "Configuration saved in ./models/squad/checkpoint-32000/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-32000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-32000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-32000/special_tokens_map.json\n",
            "{'loss': 0.7561, 'learning_rate': 6.003951968384252e-07, 'epoch': 2.96}\n",
            " 99% 32500/32895 [42:00<00:30, 13.02it/s]Saving model checkpoint to ./models/squad/checkpoint-32500\n",
            "Configuration saved in ./models/squad/checkpoint-32500/config.json\n",
            "Model weights saved in ./models/squad/checkpoint-32500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/checkpoint-32500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/checkpoint-32500/special_tokens_map.json\n",
            "100% 32895/32895 [42:31<00:00, 13.50it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2551.26, 'train_samples_per_second': 103.142, 'train_steps_per_second': 12.894, 'train_loss': 1.0716058626999734, 'epoch': 3.0}\n",
            "100% 32895/32895 [42:31<00:00, 12.89it/s]\n",
            "Saving model checkpoint to ./models/squad/\n",
            "Configuration saved in ./models/squad/config.json\n",
            "Model weights saved in ./models/squad/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SQUAD 2.0"
      ],
      "metadata": {
        "id": "PlWMg6QyveFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run.py --do_train --task qa --dataset squad_v2 --output_dir ./models/squad_v2/ "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R5_UsbbtGhW",
        "outputId": "4d5d4b5f-bb15-4121-b3ee-9cf84cdcb393"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "Moving 0 files to the new cache system\n",
            "\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n",
            "2022-12-05 01:29:53.873459: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "Downloading builder script: 100% 5.28k/5.28k [00:00<00:00, 5.28MB/s]\n",
            "Downloading metadata: 100% 2.40k/2.40k [00:00<00:00, 2.40MB/s]\n",
            "Downloading readme: 100% 8.02k/8.02k [00:00<00:00, 8.01MB/s]\n",
            "Downloading and preparing dataset squad_v2/squad_v2 to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n",
            "Downloading data files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/9.55M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:  87% 8.29M/9.55M [00:00<00:00, 82.8MB/s]\u001b[A\n",
            "Downloading data: 19.1MB [00:00, 98.0MB/s]                \u001b[A\n",
            "Downloading data: 30.3MB [00:00, 104MB/s] \u001b[A\n",
            "Downloading data: 42.1MB [00:00, 104MB/s]\n",
            "Downloading data files:  50% 1/2 [00:00<00:00,  1.76it/s]\n",
            "Downloading data: 4.37MB [00:00, 76.0MB/s]      \n",
            "Downloading data files: 100% 2/2 [00:00<00:00,  2.43it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1426.63it/s]\n",
            "Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 653.98it/s]\n",
            "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
            "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "#0:   0% 0/66 [00:00<?, ?ba/s]\n",
            "#1:   0% 0/66 [00:00<?, ?ba/s]\u001b[A\n",
            "#0:   3% 2/66 [00:01<00:54,  1.18ba/s]\n",
            "#0:   5% 3/66 [00:02<00:49,  1.28ba/s]\n",
            "#0:   6% 4/66 [00:03<00:45,  1.37ba/s]\n",
            "#0:   8% 5/66 [00:03<00:44,  1.36ba/s]\n",
            "#0:   9% 6/66 [00:04<00:42,  1.41ba/s]\n",
            "#0:  11% 7/66 [00:05<00:40,  1.47ba/s]\n",
            "#0:  12% 8/66 [00:05<00:42,  1.38ba/s]\n",
            "#0:  14% 9/66 [00:06<00:38,  1.49ba/s]\n",
            "#0:  17% 11/66 [00:07<00:34,  1.58ba/s]\n",
            "#0:  18% 12/66 [00:08<00:34,  1.58ba/s]\n",
            "#0:  20% 13/66 [00:08<00:32,  1.64ba/s]\n",
            "#0:  21% 14/66 [00:09<00:31,  1.67ba/s]\n",
            "#0:  23% 15/66 [00:10<00:33,  1.54ba/s]\n",
            "#0:  26% 17/66 [00:11<00:29,  1.63ba/s]\n",
            "#0:  27% 18/66 [00:12<00:29,  1.62ba/s]\n",
            "#0:  29% 19/66 [00:12<00:28,  1.66ba/s]\n",
            "#0:  30% 20/66 [00:13<00:27,  1.70ba/s]\n",
            "#0:  32% 21/66 [00:13<00:27,  1.66ba/s]\n",
            "#0:  33% 22/66 [00:14<00:28,  1.55ba/s]\n",
            "#0:  36% 24/66 [00:15<00:26,  1.57ba/s]\n",
            "#0:  38% 25/66 [00:16<00:26,  1.54ba/s]\n",
            "#0:  39% 26/66 [00:17<00:25,  1.56ba/s]\n",
            "#0:  41% 27/66 [00:17<00:24,  1.60ba/s]\n",
            "#0:  42% 28/66 [00:18<00:24,  1.54ba/s]\n",
            "#0:  44% 29/66 [00:19<00:25,  1.47ba/s]\n",
            "#0:  45% 30/66 [00:19<00:23,  1.51ba/s]\n",
            "#0:  47% 31/66 [00:20<00:23,  1.48ba/s]\n",
            "#0:  48% 32/66 [00:21<00:23,  1.46ba/s]\n",
            "#0:  50% 33/66 [00:21<00:22,  1.45ba/s]\n",
            "#0:  52% 34/66 [00:22<00:22,  1.44ba/s]\n",
            "#0:  53% 35/66 [00:23<00:21,  1.43ba/s]\n",
            "#0:  55% 36/66 [00:24<00:22,  1.35ba/s]\n",
            "#0:  56% 37/66 [00:24<00:21,  1.36ba/s]\n",
            "#0:  58% 38/66 [00:25<00:20,  1.36ba/s]\n",
            "#0:  59% 39/66 [00:26<00:19,  1.37ba/s]\n",
            "#0:  61% 40/66 [00:27<00:19,  1.35ba/s]\n",
            "#0:  62% 41/66 [00:27<00:18,  1.36ba/s]\n",
            "#0:  64% 42/66 [00:28<00:18,  1.29ba/s]\n",
            "#0:  65% 43/66 [00:29<00:17,  1.31ba/s]\n",
            "#0:  67% 44/66 [00:30<00:16,  1.35ba/s]\n",
            "#0:  68% 45/66 [00:30<00:15,  1.36ba/s]\n",
            "#0:  70% 46/66 [00:31<00:14,  1.36ba/s]\n",
            "#0:  71% 47/66 [00:32<00:14,  1.35ba/s]\n",
            "#0:  73% 48/66 [00:33<00:13,  1.35ba/s]\n",
            "#0:  74% 49/66 [00:33<00:13,  1.28ba/s]\n",
            "#0:  76% 50/66 [00:34<00:12,  1.30ba/s]\n",
            "#0:  77% 51/66 [00:35<00:11,  1.32ba/s]\n",
            "#0:  79% 52/66 [00:36<00:10,  1.34ba/s]\n",
            "#0:  80% 53/66 [00:36<00:09,  1.36ba/s]\n",
            "#0:  82% 54/66 [00:37<00:08,  1.37ba/s]\n",
            "#0:  83% 55/66 [00:38<00:07,  1.39ba/s]\n",
            "#0:  85% 56/66 [00:39<00:07,  1.33ba/s]\n",
            "#0:  86% 57/66 [00:39<00:06,  1.33ba/s]\n",
            "#0:  88% 58/66 [00:40<00:05,  1.34ba/s]\n",
            "#0:  89% 59/66 [00:41<00:05,  1.36ba/s]\n",
            "#0:  91% 60/66 [00:41<00:04,  1.35ba/s]\n",
            "#0:  92% 61/66 [00:42<00:03,  1.35ba/s]\n",
            "#0:  94% 62/66 [00:43<00:02,  1.36ba/s]\n",
            "#0:  95% 63/66 [00:44<00:02,  1.30ba/s]\n",
            "#0:  97% 64/66 [00:45<00:01,  1.31ba/s]\n",
            "#0:  98% 65/66 [00:45<00:00,  1.42ba/s]\n",
            "\n",
            "#1:  94% 62/66 [00:45<00:02,  1.39ba/s]\u001b[A\n",
            "#1:  95% 63/66 [00:46<00:02,  1.31ba/s]\u001b[A\n",
            "#1:  97% 64/66 [00:47<00:01,  1.33ba/s]\u001b[A\n",
            "#1:  98% 65/66 [00:48<00:00,  1.34ba/s]\n",
            "run.py:133: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = datasets.load_metric('squad')\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 130503\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 48939\n",
            "  0% 0/48939 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 3.3913, 'learning_rate': 4.9489159974662335e-05, 'epoch': 0.03}\n",
            "  1% 500/48939 [00:39<1:01:32, 13.12it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 2.1812, 'learning_rate': 4.897831994932467e-05, 'epoch': 0.06}\n",
            "  2% 1000/48939 [01:18<1:00:54, 13.12it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-1000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-1000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 1.8878, 'learning_rate': 4.846747992398701e-05, 'epoch': 0.09}\n",
            "  3% 1500/48939 [01:56<1:00:12, 13.13it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-1500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-1500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 1.761, 'learning_rate': 4.795663989864934e-05, 'epoch': 0.12}\n",
            "  4% 2000/48939 [02:35<59:52, 13.07it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-2000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-2000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 1.7405, 'learning_rate': 4.744579987331167e-05, 'epoch': 0.15}\n",
            "  5% 2500/48939 [03:14<58:53, 13.14it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-2500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-2500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 1.6054, 'learning_rate': 4.6934959847974006e-05, 'epoch': 0.18}\n",
            "  6% 3000/48939 [03:53<58:34, 13.07it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-3000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-3000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 1.5942, 'learning_rate': 4.6424119822636345e-05, 'epoch': 0.21}\n",
            "  7% 3500/48939 [04:32<58:10, 13.02it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-3500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-3500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 1.6006, 'learning_rate': 4.591327979729868e-05, 'epoch': 0.25}\n",
            "  8% 4000/48939 [05:11<58:02, 12.90it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-4000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-4000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 1.5382, 'learning_rate': 4.540243977196101e-05, 'epoch': 0.28}\n",
            "  9% 4500/48939 [05:50<56:51, 13.02it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-4500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-4500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 1.5061, 'learning_rate': 4.489159974662335e-05, 'epoch': 0.31}\n",
            " 10% 5000/48939 [06:29<55:52, 13.11it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-5000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-5000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-5000/special_tokens_map.json\n",
            "{'loss': 1.5151, 'learning_rate': 4.438075972128568e-05, 'epoch': 0.34}\n",
            " 11% 5500/48939 [07:07<55:15, 13.10it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-5500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-5500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-5500/special_tokens_map.json\n",
            "{'loss': 1.4547, 'learning_rate': 4.386991969594802e-05, 'epoch': 0.37}\n",
            " 12% 6000/48939 [07:46<54:21, 13.16it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-6000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-6000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-6000/special_tokens_map.json\n",
            "{'loss': 1.4398, 'learning_rate': 4.3359079670610356e-05, 'epoch': 0.4}\n",
            " 13% 6500/48939 [08:25<54:04, 13.08it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-6500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-6500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-6500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-6500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-6500/special_tokens_map.json\n",
            "{'loss': 1.4221, 'learning_rate': 4.284823964527269e-05, 'epoch': 0.43}\n",
            " 14% 7000/48939 [09:04<53:13, 13.13it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-7000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-7000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-7000/special_tokens_map.json\n",
            "{'loss': 1.4065, 'learning_rate': 4.233739961993503e-05, 'epoch': 0.46}\n",
            " 15% 7500/48939 [09:43<53:47, 12.84it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-7500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-7500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-7500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-7500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-7500/special_tokens_map.json\n",
            "{'loss': 1.374, 'learning_rate': 4.182655959459736e-05, 'epoch': 0.49}\n",
            " 16% 8000/48939 [10:22<52:09, 13.08it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-8000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-8000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-8000/special_tokens_map.json\n",
            "{'loss': 1.3801, 'learning_rate': 4.1315719569259694e-05, 'epoch': 0.52}\n",
            " 17% 8500/48939 [11:01<51:46, 13.02it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-8500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-8500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-8500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-8500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-8500/special_tokens_map.json\n",
            "{'loss': 1.3132, 'learning_rate': 4.0804879543922026e-05, 'epoch': 0.55}\n",
            " 18% 9000/48939 [11:40<51:07, 13.02it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-9000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-9000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-9000/special_tokens_map.json\n",
            "{'loss': 1.3445, 'learning_rate': 4.0294039518584366e-05, 'epoch': 0.58}\n",
            " 19% 9500/48939 [12:19<50:26, 13.03it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-9500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-9500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-9500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-9500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-9500/special_tokens_map.json\n",
            "{'loss': 1.3279, 'learning_rate': 3.97831994932467e-05, 'epoch': 0.61}\n",
            " 20% 10000/48939 [12:58<49:51, 13.02it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-10000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-10000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-10000/special_tokens_map.json\n",
            "{'loss': 1.3349, 'learning_rate': 3.927235946790903e-05, 'epoch': 0.64}\n",
            " 21% 10500/48939 [13:37<49:14, 13.01it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-10500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-10500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-10500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-10500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-10500/special_tokens_map.json\n",
            "{'loss': 1.2898, 'learning_rate': 3.8761519442571364e-05, 'epoch': 0.67}\n",
            " 22% 11000/48939 [14:16<48:13, 13.11it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-11000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-11000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-11000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-11000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-11000/special_tokens_map.json\n",
            "{'loss': 1.3049, 'learning_rate': 3.8250679417233704e-05, 'epoch': 0.7}\n",
            " 23% 11500/48939 [14:55<47:43, 13.07it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-11500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-11500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-11500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-11500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-11500/special_tokens_map.json\n",
            "{'loss': 1.2908, 'learning_rate': 3.7739839391896037e-05, 'epoch': 0.74}\n",
            " 25% 12000/48939 [15:33<46:44, 13.17it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-12000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-12000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-12000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-12000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-12000/special_tokens_map.json\n",
            "{'loss': 1.2725, 'learning_rate': 3.722899936655837e-05, 'epoch': 0.77}\n",
            " 26% 12500/48939 [16:12<46:28, 13.07it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-12500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-12500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-12500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-12500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-12500/special_tokens_map.json\n",
            "{'loss': 1.2396, 'learning_rate': 3.67181593412207e-05, 'epoch': 0.8}\n",
            " 27% 13000/48939 [16:51<46:17, 12.94it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-13000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-13000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-13000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-13000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-13000/special_tokens_map.json\n",
            "{'loss': 1.2422, 'learning_rate': 3.6207319315883035e-05, 'epoch': 0.83}\n",
            " 28% 13500/48939 [17:30<45:44, 12.91it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-13500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-13500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-13500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-13500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-13500/special_tokens_map.json\n",
            "{'loss': 1.2621, 'learning_rate': 3.5696479290545374e-05, 'epoch': 0.86}\n",
            " 29% 14000/48939 [18:10<45:53, 12.69it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-14000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-14000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-14000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-14000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-14000/special_tokens_map.json\n",
            "{'loss': 1.2287, 'learning_rate': 3.518563926520771e-05, 'epoch': 0.89}\n",
            " 30% 14500/48939 [18:49<44:34, 12.87it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-14500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-14500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-14500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-14500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-14500/special_tokens_map.json\n",
            "{'loss': 1.2205, 'learning_rate': 3.467479923987004e-05, 'epoch': 0.92}\n",
            " 31% 15000/48939 [19:29<43:36, 12.97it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-15000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-15000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-15000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-15000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-15000/special_tokens_map.json\n",
            "{'loss': 1.1961, 'learning_rate': 3.416395921453237e-05, 'epoch': 0.95}\n",
            " 32% 15500/48939 [20:09<43:38, 12.77it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-15500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-15500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-15500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-15500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-15500/special_tokens_map.json\n",
            "{'loss': 1.2097, 'learning_rate': 3.365311918919471e-05, 'epoch': 0.98}\n",
            " 33% 16000/48939 [20:49<42:56, 12.78it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-16000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-16000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-16000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-16000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-16000/special_tokens_map.json\n",
            "{'loss': 1.1132, 'learning_rate': 3.3142279163857045e-05, 'epoch': 1.01}\n",
            " 34% 16500/48939 [21:28<42:10, 12.82it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-16500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-16500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-16500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-16500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-16500/special_tokens_map.json\n",
            "{'loss': 1.0056, 'learning_rate': 3.2631439138519385e-05, 'epoch': 1.04}\n",
            " 35% 17000/48939 [22:08<41:09, 12.93it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-17000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-17000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-17000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-17000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-17000/special_tokens_map.json\n",
            "{'loss': 1.045, 'learning_rate': 3.212059911318172e-05, 'epoch': 1.07}\n",
            " 36% 17500/48939 [22:48<40:20, 12.99it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-17500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-17500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-17500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-17500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-17500/special_tokens_map.json\n",
            "{'loss': 0.9987, 'learning_rate': 3.160975908784406e-05, 'epoch': 1.1}\n",
            " 37% 18000/48939 [23:27<39:53, 12.93it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-18000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-18000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-18000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-18000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-18000/special_tokens_map.json\n",
            "{'loss': 0.9977, 'learning_rate': 3.109891906250639e-05, 'epoch': 1.13}\n",
            " 38% 18500/48939 [24:07<39:45, 12.76it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-18500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-18500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-18500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-18500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-18500/special_tokens_map.json\n",
            "{'loss': 1.0451, 'learning_rate': 3.058807903716872e-05, 'epoch': 1.16}\n",
            " 39% 19000/48939 [24:46<38:22, 13.00it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-19000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-19000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-19000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-19000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-19000/special_tokens_map.json\n",
            "{'loss': 1.043, 'learning_rate': 3.0077239011831055e-05, 'epoch': 1.2}\n",
            " 40% 19500/48939 [25:25<37:29, 13.09it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-19500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-19500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-19500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-19500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-19500/special_tokens_map.json\n",
            "{'loss': 1.0743, 'learning_rate': 2.9566398986493395e-05, 'epoch': 1.23}\n",
            " 41% 20000/48939 [26:04<36:45, 13.12it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-20000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-20000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-20000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-20000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-20000/special_tokens_map.json\n",
            "{'loss': 1.0361, 'learning_rate': 2.9055558961155728e-05, 'epoch': 1.26}\n",
            " 42% 20500/48939 [26:43<36:28, 13.00it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-20500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-20500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-20500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-20500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-20500/special_tokens_map.json\n",
            "{'loss': 1.0519, 'learning_rate': 2.854471893581806e-05, 'epoch': 1.29}\n",
            " 43% 21000/48939 [27:22<35:51, 12.98it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-21000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-21000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-21000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-21000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-21000/special_tokens_map.json\n",
            "{'loss': 1.0047, 'learning_rate': 2.8033878910480393e-05, 'epoch': 1.32}\n",
            " 44% 21500/48939 [28:01<35:53, 12.74it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-21500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-21500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-21500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-21500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-21500/special_tokens_map.json\n",
            "{'loss': 0.9834, 'learning_rate': 2.7523038885142733e-05, 'epoch': 1.35}\n",
            " 45% 22000/48939 [28:39<34:08, 13.15it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-22000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-22000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-22000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-22000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-22000/special_tokens_map.json\n",
            "{'loss': 0.9984, 'learning_rate': 2.7012198859805066e-05, 'epoch': 1.38}\n",
            " 46% 22500/48939 [29:18<33:45, 13.05it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-22500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-22500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-22500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-22500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-22500/special_tokens_map.json\n",
            "{'loss': 1.017, 'learning_rate': 2.65013588344674e-05, 'epoch': 1.41}\n",
            " 47% 23000/48939 [29:57<33:36, 12.86it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-23000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-23000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-23000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-23000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-23000/special_tokens_map.json\n",
            "{'loss': 1.018, 'learning_rate': 2.599051880912973e-05, 'epoch': 1.44}\n",
            " 48% 23500/48939 [30:36<32:42, 12.96it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-23500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-23500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-23500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-23500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-23500/special_tokens_map.json\n",
            "{'loss': 0.9826, 'learning_rate': 2.547967878379207e-05, 'epoch': 1.47}\n",
            " 49% 24000/48939 [31:15<32:10, 12.92it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-24000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-24000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-24000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-24000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-24000/special_tokens_map.json\n",
            "{'loss': 1.0171, 'learning_rate': 2.4968838758454403e-05, 'epoch': 1.5}\n",
            " 50% 24500/48939 [31:55<30:58, 13.15it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-24500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-24500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-24500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-24500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-24500/special_tokens_map.json\n",
            "{'loss': 1.0042, 'learning_rate': 2.445799873311674e-05, 'epoch': 1.53}\n",
            " 51% 25000/48939 [32:33<30:21, 13.14it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-25000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-25000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-25000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-25000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-25000/special_tokens_map.json\n",
            "{'loss': 1.0081, 'learning_rate': 2.3947158707779076e-05, 'epoch': 1.56}\n",
            " 52% 25500/48939 [33:12<29:43, 13.14it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-25500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-25500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-25500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-25500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-25500/special_tokens_map.json\n",
            "{'loss': 1.0203, 'learning_rate': 2.343631868244141e-05, 'epoch': 1.59}\n",
            " 53% 26000/48939 [33:51<28:58, 13.19it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-26000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-26000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-26000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-26000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-26000/special_tokens_map.json\n",
            "{'loss': 0.9932, 'learning_rate': 2.2925478657103745e-05, 'epoch': 1.62}\n",
            " 54% 26500/48939 [34:29<28:25, 13.16it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-26500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-26500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-26500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-26500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-26500/special_tokens_map.json\n",
            "{'loss': 0.9869, 'learning_rate': 2.2414638631766077e-05, 'epoch': 1.66}\n",
            " 55% 27000/48939 [35:08<27:42, 13.20it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-27000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-27000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-27000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-27000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-27000/special_tokens_map.json\n",
            "{'loss': 1.0031, 'learning_rate': 2.190379860642841e-05, 'epoch': 1.69}\n",
            " 56% 27500/48939 [35:47<27:05, 13.19it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-27500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-27500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-27500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-27500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-27500/special_tokens_map.json\n",
            "{'loss': 0.9792, 'learning_rate': 2.1392958581090746e-05, 'epoch': 1.72}\n",
            " 57% 28000/48939 [36:25<26:37, 13.11it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-28000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-28000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-28000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-28000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-28000/special_tokens_map.json\n",
            "{'loss': 0.966, 'learning_rate': 2.088211855575308e-05, 'epoch': 1.75}\n",
            " 58% 28500/48939 [37:04<25:47, 13.21it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-28500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-28500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-28500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-28500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-28500/special_tokens_map.json\n",
            "{'loss': 0.9308, 'learning_rate': 2.0371278530415415e-05, 'epoch': 1.78}\n",
            " 59% 29000/48939 [37:43<25:23, 13.08it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-29000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-29000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-29000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-29000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-29000/special_tokens_map.json\n",
            "{'loss': 0.9641, 'learning_rate': 1.9860438505077748e-05, 'epoch': 1.81}\n",
            " 60% 29500/48939 [38:22<24:40, 13.13it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-29500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-29500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-29500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-29500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-29500/special_tokens_map.json\n",
            "{'loss': 0.939, 'learning_rate': 1.9349598479740084e-05, 'epoch': 1.84}\n",
            " 61% 30000/48939 [39:01<24:21, 12.96it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-30000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-30000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-30000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-30000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-30000/special_tokens_map.json\n",
            "{'loss': 0.9877, 'learning_rate': 1.883875845440242e-05, 'epoch': 1.87}\n",
            " 62% 30500/48939 [39:40<23:25, 13.12it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-30500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-30500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-30500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-30500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-30500/special_tokens_map.json\n",
            "{'loss': 0.9783, 'learning_rate': 1.8327918429064757e-05, 'epoch': 1.9}\n",
            " 63% 31000/48939 [40:18<22:45, 13.14it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-31000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-31000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-31000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-31000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-31000/special_tokens_map.json\n",
            "{'loss': 0.9455, 'learning_rate': 1.781707840372709e-05, 'epoch': 1.93}\n",
            " 64% 31500/48939 [40:57<22:12, 13.09it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-31500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-31500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-31500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-31500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-31500/special_tokens_map.json\n",
            "{'loss': 0.9576, 'learning_rate': 1.7306238378389426e-05, 'epoch': 1.96}\n",
            " 65% 32000/48939 [41:36<21:32, 13.10it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-32000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-32000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-32000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-32000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-32000/special_tokens_map.json\n",
            "{'loss': 0.9555, 'learning_rate': 1.679539835305176e-05, 'epoch': 1.99}\n",
            " 66% 32500/48939 [42:15<20:54, 13.11it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-32500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-32500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-32500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-32500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-32500/special_tokens_map.json\n",
            "{'loss': 0.854, 'learning_rate': 1.6284558327714095e-05, 'epoch': 2.02}\n",
            " 67% 33000/48939 [42:54<20:22, 13.03it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-33000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-33000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-33000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-33000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-33000/special_tokens_map.json\n",
            "{'loss': 0.802, 'learning_rate': 1.5773718302376427e-05, 'epoch': 2.05}\n",
            " 68% 33500/48939 [43:33<19:35, 13.13it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-33500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-33500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-33500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-33500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-33500/special_tokens_map.json\n",
            "{'loss': 0.7873, 'learning_rate': 1.5262878277038763e-05, 'epoch': 2.08}\n",
            " 69% 34000/48939 [44:12<18:57, 13.14it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-34000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-34000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-34000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-34000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-34000/special_tokens_map.json\n",
            "{'loss': 0.8015, 'learning_rate': 1.4752038251701098e-05, 'epoch': 2.11}\n",
            " 70% 34500/48939 [44:50<18:16, 13.16it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-34500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-34500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-34500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-34500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-34500/special_tokens_map.json\n",
            "{'loss': 0.7822, 'learning_rate': 1.4241198226363434e-05, 'epoch': 2.15}\n",
            " 72% 35000/48939 [45:29<17:39, 13.16it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-35000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-35000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-35000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-35000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-35000/special_tokens_map.json\n",
            "{'loss': 0.8274, 'learning_rate': 1.3730358201025767e-05, 'epoch': 2.18}\n",
            " 73% 35500/48939 [46:08<17:00, 13.17it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-35500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-35500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-35500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-35500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-35500/special_tokens_map.json\n",
            "{'loss': 0.8066, 'learning_rate': 1.3219518175688103e-05, 'epoch': 2.21}\n",
            " 74% 36000/48939 [46:47<16:22, 13.17it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-36000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-36000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-36000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-36000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-36000/special_tokens_map.json\n",
            "{'loss': 0.8247, 'learning_rate': 1.2708678150350436e-05, 'epoch': 2.24}\n",
            " 75% 36500/48939 [47:25<15:43, 13.18it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-36500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-36500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-36500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-36500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-36500/special_tokens_map.json\n",
            "{'loss': 0.7953, 'learning_rate': 1.2197838125012772e-05, 'epoch': 2.27}\n",
            " 76% 37000/48939 [48:04<15:06, 13.17it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-37000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-37000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-37000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-37000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-37000/special_tokens_map.json\n",
            "{'loss': 0.7652, 'learning_rate': 1.1686998099675107e-05, 'epoch': 2.3}\n",
            " 77% 37500/48939 [48:43<14:27, 13.19it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-37500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-37500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-37500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-37500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-37500/special_tokens_map.json\n",
            "{'loss': 0.7694, 'learning_rate': 1.1176158074337441e-05, 'epoch': 2.33}\n",
            " 78% 38000/48939 [49:22<13:56, 13.08it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-38000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-38000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-38000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-38000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-38000/special_tokens_map.json\n",
            "{'loss': 0.7947, 'learning_rate': 1.0665318048999775e-05, 'epoch': 2.36}\n",
            " 79% 38500/48939 [50:00<13:13, 13.16it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-38500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-38500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-38500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-38500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-38500/special_tokens_map.json\n",
            "{'loss': 0.7886, 'learning_rate': 1.015447802366211e-05, 'epoch': 2.39}\n",
            " 80% 39000/48939 [50:39<12:37, 13.12it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-39000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-39000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-39000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-39000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-39000/special_tokens_map.json\n",
            "{'loss': 0.7757, 'learning_rate': 9.643637998324444e-06, 'epoch': 2.42}\n",
            " 81% 39500/48939 [51:18<11:57, 13.15it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-39500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-39500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-39500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-39500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-39500/special_tokens_map.json\n",
            "{'loss': 0.7784, 'learning_rate': 9.13279797298678e-06, 'epoch': 2.45}\n",
            " 82% 40000/48939 [51:56<11:20, 13.14it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-40000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-40000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-40000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-40000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-40000/special_tokens_map.json\n",
            "{'loss': 0.7567, 'learning_rate': 8.621957947649115e-06, 'epoch': 2.48}\n",
            " 83% 40500/48939 [52:35<10:40, 13.18it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-40500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-40500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-40500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-40500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-40500/special_tokens_map.json\n",
            "{'loss': 0.7941, 'learning_rate': 8.11111792231145e-06, 'epoch': 2.51}\n",
            " 84% 41000/48939 [53:14<10:02, 13.17it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-41000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-41000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-41000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-41000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-41000/special_tokens_map.json\n",
            "{'loss': 0.819, 'learning_rate': 7.600277896973784e-06, 'epoch': 2.54}\n",
            " 85% 41500/48939 [53:52<09:26, 13.13it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-41500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-41500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-41500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-41500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-41500/special_tokens_map.json\n",
            "{'loss': 0.7736, 'learning_rate': 7.089437871636119e-06, 'epoch': 2.57}\n",
            " 86% 42000/48939 [54:31<08:53, 13.02it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-42000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-42000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-42000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-42000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-42000/special_tokens_map.json\n",
            "{'loss': 0.7535, 'learning_rate': 6.578597846298454e-06, 'epoch': 2.61}\n",
            " 87% 42500/48939 [55:10<08:10, 13.12it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-42500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-42500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-42500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-42500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-42500/special_tokens_map.json\n",
            "{'loss': 0.7953, 'learning_rate': 6.067757820960788e-06, 'epoch': 2.64}\n",
            " 88% 43000/48939 [55:49<07:34, 13.08it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-43000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-43000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-43000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-43000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-43000/special_tokens_map.json\n",
            "{'loss': 0.8237, 'learning_rate': 5.556917795623123e-06, 'epoch': 2.67}\n",
            " 89% 43500/48939 [56:27<07:02, 12.87it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-43500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-43500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-43500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-43500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-43500/special_tokens_map.json\n",
            "{'loss': 0.7723, 'learning_rate': 5.046077770285457e-06, 'epoch': 2.7}\n",
            " 90% 44000/48939 [57:06<06:17, 13.08it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-44000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-44000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-44000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-44000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-44000/special_tokens_map.json\n",
            "{'loss': 0.7658, 'learning_rate': 4.5352377449477925e-06, 'epoch': 2.73}\n",
            " 91% 44500/48939 [57:45<05:43, 12.93it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-44500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-44500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-44500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-44500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-44500/special_tokens_map.json\n",
            "{'loss': 0.7725, 'learning_rate': 4.024397719610127e-06, 'epoch': 2.76}\n",
            " 92% 45000/48939 [58:24<05:00, 13.12it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-45000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-45000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-45000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-45000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-45000/special_tokens_map.json\n",
            "{'loss': 0.7366, 'learning_rate': 3.513557694272462e-06, 'epoch': 2.79}\n",
            " 93% 45500/48939 [59:03<04:24, 12.99it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-45500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-45500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-45500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-45500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-45500/special_tokens_map.json\n",
            "{'loss': 0.7747, 'learning_rate': 3.0027176689347964e-06, 'epoch': 2.82}\n",
            " 94% 46000/48939 [59:41<03:43, 13.14it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-46000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-46000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-46000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-46000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-46000/special_tokens_map.json\n",
            "{'loss': 0.7954, 'learning_rate': 2.4918776435971313e-06, 'epoch': 2.85}\n",
            " 95% 46500/48939 [1:00:20<03:05, 13.17it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-46500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-46500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-46500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-46500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-46500/special_tokens_map.json\n",
            "{'loss': 0.7761, 'learning_rate': 1.981037618259466e-06, 'epoch': 2.88}\n",
            " 96% 47000/48939 [1:00:59<02:27, 13.18it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-47000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-47000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-47000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-47000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-47000/special_tokens_map.json\n",
            "{'loss': 0.7537, 'learning_rate': 1.4701975929218006e-06, 'epoch': 2.91}\n",
            " 97% 47500/48939 [1:01:37<01:49, 13.14it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-47500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-47500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-47500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-47500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-47500/special_tokens_map.json\n",
            "{'loss': 0.7612, 'learning_rate': 9.593575675841353e-07, 'epoch': 2.94}\n",
            " 98% 48000/48939 [1:02:16<01:11, 13.12it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-48000\n",
            "Configuration saved in ./models/squad_v2/checkpoint-48000/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-48000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-48000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-48000/special_tokens_map.json\n",
            "{'loss': 0.7563, 'learning_rate': 4.4851754224647015e-07, 'epoch': 2.97}\n",
            " 99% 48500/48939 [1:02:55<00:33, 13.03it/s]Saving model checkpoint to ./models/squad_v2/checkpoint-48500\n",
            "Configuration saved in ./models/squad_v2/checkpoint-48500/config.json\n",
            "Model weights saved in ./models/squad_v2/checkpoint-48500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/checkpoint-48500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/checkpoint-48500/special_tokens_map.json\n",
            "100% 48939/48939 [1:03:29<00:00, 12.89it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3809.4284, 'train_samples_per_second': 102.774, 'train_steps_per_second': 12.847, 'train_loss': 1.090607098524239, 'epoch': 3.0}\n",
            "100% 48939/48939 [1:03:29<00:00, 12.85it/s]\n",
            "Saving model checkpoint to ./models/squad_v2/\n",
            "Configuration saved in ./models/squad_v2/config.json\n",
            "Model weights saved in ./models/squad_v2/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_v2/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_v2/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SQUAD Adversarial"
      ],
      "metadata": {
        "id": "SnuyyTb9vhQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AddSent\n"
      ],
      "metadata": {
        "id": "_FEPW7vivlFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_squad_adv.py --do_train --task qa --dataset squad_adversarial:AddSent --output_dir ./models/squad_adv/addsent/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmLGFqjL6j6U",
        "outputId": "4a1870d3-1ce2-40dc-d045-4bcf661d3452"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-05 02:48:33.642036: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING:datasets.builder:Found cached dataset squad_adversarial (/root/.cache/huggingface/datasets/squad_adversarial/AddSent/1.1.0/e9df92c060f50eb529284b303c504bf4359ba37944faebe7a16a91b7d534e946)\n",
            "100% 1/1 [00:00<00:00, 602.46it/s]\n",
            "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']\n",
            "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "#0:   0% 0/2 [00:00<?, ?ba/s]\n",
            "#0:  50% 1/2 [00:00<00:00,  1.31ba/s]\n",
            "#0:  50% 1/2 [00:01<00:01,  1.43s/ba]\n",
            "#1:  50% 1/2 [00:01<00:01,  1.47s/ba]\n",
            "run_squad_adv.py:133: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = datasets.load_metric('squad')\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 3591\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1347\n",
            "  0% 0/1347 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 2.9658, 'learning_rate': 3.144023756495917e-05, 'epoch': 1.11}\n",
            " 37% 500/1347 [00:38<01:03, 13.31it/s]Saving model checkpoint to ./models/squad_adv/addsent/checkpoint-500\n",
            "Configuration saved in ./models/squad_adv/addsent/checkpoint-500/config.json\n",
            "Model weights saved in ./models/squad_adv/addsent/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_adv/addsent/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_adv/addsent/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 1.04, 'learning_rate': 1.2880475129918337e-05, 'epoch': 2.23}\n",
            " 74% 1000/1347 [01:16<00:26, 13.29it/s]Saving model checkpoint to ./models/squad_adv/addsent/checkpoint-1000\n",
            "Configuration saved in ./models/squad_adv/addsent/checkpoint-1000/config.json\n",
            "Model weights saved in ./models/squad_adv/addsent/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_adv/addsent/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_adv/addsent/checkpoint-1000/special_tokens_map.json\n",
            "100% 1347/1347 [01:43<00:00, 13.26it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 103.6601, 'train_samples_per_second': 103.926, 'train_steps_per_second': 12.994, 'train_loss': 1.6439035892840572, 'epoch': 3.0}\n",
            "100% 1347/1347 [01:43<00:00, 13.00it/s]\n",
            "Saving model checkpoint to ./models/squad_adv/addsent/\n",
            "Configuration saved in ./models/squad_adv/addsent/config.json\n",
            "Model weights saved in ./models/squad_adv/addsent/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_adv/addsent/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_adv/addsent/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AddOneSent"
      ],
      "metadata": {
        "id": "WUrFp0Usvo6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_squad_adv.py --do_train --task qa --dataset squad_adversarial:AddOneSent --output_dir ./models/squad_adv/addonesent/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGTRp51RujQi",
        "outputId": "6a94a42a-b996-4702-a63d-fa63c44e2672"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-05 02:50:30.027781: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING:datasets.builder:Found cached dataset squad_adversarial (/root/.cache/huggingface/datasets/squad_adversarial/AddOneSent/1.1.0/e9df92c060f50eb529284b303c504bf4359ba37944faebe7a16a91b7d534e946)\n",
            "100% 1/1 [00:00<00:00, 617.99it/s]\n",
            "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
            "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "#0:   0% 0/1 [00:00<?, ?ba/s]\n",
            "#0:   0% 0/1 [00:00<?, ?ba/s]\n",
            "#1:   0% 0/1 [00:00<?, ?ba/s]\n",
            "run_squad_adv.py:133: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = datasets.load_metric('squad')\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 1803\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 678\n",
            "  0% 0/678 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 3.0187, 'learning_rate': 1.3126843657817109e-05, 'epoch': 2.21}\n",
            " 74% 500/678 [00:38<00:13, 13.31it/s]Saving model checkpoint to ./models/squad_adv/addonesent/checkpoint-500\n",
            "Configuration saved in ./models/squad_adv/addonesent/checkpoint-500/config.json\n",
            "Model weights saved in ./models/squad_adv/addonesent/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_adv/addonesent/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_adv/addonesent/checkpoint-500/special_tokens_map.json\n",
            "100% 677/678 [00:52<00:00, 13.32it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 52.7971, 'train_samples_per_second': 102.449, 'train_steps_per_second': 12.842, 'train_loss': 2.666765241144681, 'epoch': 3.0}\n",
            "100% 678/678 [00:52<00:00, 12.85it/s]\n",
            "Saving model checkpoint to ./models/squad_adv/addonesent/\n",
            "Configuration saved in ./models/squad_adv/addonesent/config.json\n",
            "Model weights saved in ./models/squad_adv/addonesent/pytorch_model.bin\n",
            "tokenizer config file saved in ./models/squad_adv/addonesent/tokenizer_config.json\n",
            "Special tokens file saved in ./models/squad_adv/addonesent/special_tokens_map.json\n"
          ]
        }
      ]
    }
  ]
}